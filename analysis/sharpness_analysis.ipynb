{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "a7cb6f09-93b0-4f14-8eda-d02c96688cef"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pprint\n",
    "from tensorboardX import SummaryWriter\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from numpy.linalg import svd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy\n",
    "\n",
    "sys.path = ['..'] + sys.path\n",
    "from algorithm_trainer.models import gated_conv_net_original, resnet, resnet_2, resnet_12, wide_resnet\n",
    "from algorithm_trainer.algorithm_trainer import Generic_adaptation_trainer, Classical_algorithm_trainer\n",
    "from algorithm_trainer.algorithms.algorithm import SVM, ProtoNet, Finetune, ProtoCosineNet, ProtoCosineNetCorrected2\n",
    "from algorithm_trainer.utils import accuracy\n",
    "from data_layer.dataset_managers import MetaDataManager, ClassicalDataManager\n",
    "from analysis.objectives import var_reduction_disc, var_reduction_disc_perp, var_reduction\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "46b9b5af-b3c0-4678-b41d-b5329efd1819"
    }
   },
   "source": [
    "### load tl and ml checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "638d9c11-0786-4aba-92a2-6e9bf7f12bfd"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(checkpoint, lambd):\n",
    "    \n",
    "    model_dc = resnet_12.resnet12(\n",
    "    avg_pool=True, drop_rate=0.1, dropblock_size=5,\n",
    "    classifier_type='avg-classifier', num_classes=64)\n",
    "    \n",
    "    print(f\"loading from {checkpoint}\")\n",
    "    model_dict = model_dc.state_dict()\n",
    "    chkpt_state_dict = torch.load(checkpoint)\n",
    "    if 'model' in chkpt_state_dict:\n",
    "        chkpt_state_dict = chkpt_state_dict['model']\n",
    "    chkpt_state_dict_cpy = chkpt_state_dict.copy()\n",
    "    # remove \"module.\" from key, possibly present as it was dumped by data-parallel\n",
    "    for key in chkpt_state_dict_cpy.keys():\n",
    "        if 'module.' in key:\n",
    "            new_key = re.sub('module\\.', '',  key)\n",
    "            chkpt_state_dict[new_key] = chkpt_state_dict.pop(key)\n",
    "    chkpt_state_dict = {k: v for k, v in chkpt_state_dict.items() if k in model_dict}\n",
    "    model_dict.update(chkpt_state_dict)\n",
    "    updated_keys = set(model_dict).intersection(set(chkpt_state_dict))\n",
    "    print(f\"Updated {len(updated_keys)} keys using chkpt\")\n",
    "    print(\"Following keys updated :\", \"\\n\".join(sorted(updated_keys)))\n",
    "    missed_keys = set(model_dict).difference(set(chkpt_state_dict))\n",
    "    print(f\"Missed {len(missed_keys)} keys\")\n",
    "    print(\"Following keys missed :\", \"\\n\".join(sorted(missed_keys)))\n",
    "    model_dc.load_state_dict(model_dict)\n",
    "\n",
    "#     model_dc.scale = torch.nn.Parameter(torch.tensor([1.0]))        \n",
    "    model_dc = torch.nn.DataParallel(model_dc, device_ids=range(torch.cuda.device_count()))\n",
    "    model_dc.cuda()\n",
    "    model_dc.eval()\n",
    "\n",
    "    # set lambd \n",
    "    model_dc.module.fc.lambd = lambd\n",
    "    return model_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "4c2fac95-dc09-47f9-9b75-a3b83686f544"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from ../train_dir/minmax_MI_r12_n64_s3_q128/classical_resnet_120.pt\n",
      "Updated 98 keys using chkpt\n",
      "Following keys updated : fc.scale_factor\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.bn1.num_batches_tracked\n",
      "layer1.0.bn1.running_mean\n",
      "layer1.0.bn1.running_var\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.0.bn2.num_batches_tracked\n",
      "layer1.0.bn2.running_mean\n",
      "layer1.0.bn2.running_var\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn3.bias\n",
      "layer1.0.bn3.num_batches_tracked\n",
      "layer1.0.bn3.running_mean\n",
      "layer1.0.bn3.running_var\n",
      "layer1.0.bn3.weight\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.conv3.weight\n",
      "layer1.0.downsample.0.weight\n",
      "layer1.0.downsample.1.bias\n",
      "layer1.0.downsample.1.num_batches_tracked\n",
      "layer1.0.downsample.1.running_mean\n",
      "layer1.0.downsample.1.running_var\n",
      "layer1.0.downsample.1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.bn1.num_batches_tracked\n",
      "layer2.0.bn1.running_mean\n",
      "layer2.0.bn1.running_var\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.bn2.num_batches_tracked\n",
      "layer2.0.bn2.running_mean\n",
      "layer2.0.bn2.running_var\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn3.bias\n",
      "layer2.0.bn3.num_batches_tracked\n",
      "layer2.0.bn3.running_mean\n",
      "layer2.0.bn3.running_var\n",
      "layer2.0.bn3.weight\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.conv3.weight\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.0.downsample.1.bias\n",
      "layer2.0.downsample.1.num_batches_tracked\n",
      "layer2.0.downsample.1.running_mean\n",
      "layer2.0.downsample.1.running_var\n",
      "layer2.0.downsample.1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.bn1.num_batches_tracked\n",
      "layer3.0.bn1.running_mean\n",
      "layer3.0.bn1.running_var\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.bn2.num_batches_tracked\n",
      "layer3.0.bn2.running_mean\n",
      "layer3.0.bn2.running_var\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn3.bias\n",
      "layer3.0.bn3.num_batches_tracked\n",
      "layer3.0.bn3.running_mean\n",
      "layer3.0.bn3.running_var\n",
      "layer3.0.bn3.weight\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.conv3.weight\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer3.0.downsample.1.num_batches_tracked\n",
      "layer3.0.downsample.1.running_mean\n",
      "layer3.0.downsample.1.running_var\n",
      "layer3.0.downsample.1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.bn1.num_batches_tracked\n",
      "layer4.0.bn1.running_mean\n",
      "layer4.0.bn1.running_var\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.bn2.num_batches_tracked\n",
      "layer4.0.bn2.running_mean\n",
      "layer4.0.bn2.running_var\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn3.bias\n",
      "layer4.0.bn3.num_batches_tracked\n",
      "layer4.0.bn3.running_mean\n",
      "layer4.0.bn3.running_var\n",
      "layer4.0.bn3.weight\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.conv3.weight\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.bias\n",
      "layer4.0.downsample.1.num_batches_tracked\n",
      "layer4.0.downsample.1.running_mean\n",
      "layer4.0.downsample.1.running_var\n",
      "layer4.0.downsample.1.weight\n",
      "scale\n",
      "Missed 1 keys\n",
      "Following keys missed : fc.Lg.weight\n"
     ]
    }
   ],
   "source": [
    "checkpoint_ml = '../train_dir/minmax_MI_r12_n64_s3_q128/classical_resnet_120.pt'\n",
    "# checkpoint_tl = '../train_dir/classical_miniimagenet_r12_classifier_bs128_parametric_tfl_gradanalysis/classical_resnet_399.pt'\n",
    "model_ml = load_model(checkpoint_ml, 0.)\n",
    "# model_tl = load_model(checkpoint_tl, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dabcc949-87ca-436a-bba2-8e477994c42a"
    }
   },
   "source": [
    "### interpolate b/w tl and ml checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "c7b9c6da-2e77-46f0-b44e-3a620d9a3abf"
    }
   },
   "outputs": [],
   "source": [
    "def func(x, n_param, model):\n",
    "    \n",
    "    print(\"Accessing function value\")\n",
    "    \n",
    "    \n",
    "    # dataloaders\n",
    "    image_size = 84\n",
    "    dataset_path = '../data/filelists/miniImagenet'\n",
    "    val_file = os.path.join(dataset_path, 'val.json')\n",
    "    train_file = os.path.join(dataset_path, 'base.json')\n",
    "    classical_val_datamgr = ClassicalDataManager(image_size, batch_size=128)\n",
    "    classical_val_loader = classical_val_datamgr.get_data_loader(val_file, aug=False)\n",
    "    classical_train_datamgr = ClassicalDataManager(image_size, batch_size=128)\n",
    "    classical_train_loader = classical_val_datamgr.get_data_loader(train_file, aug=True)\n",
    "    aux_datamgr = ClassicalDataManager(image_size, batch_size=128)\n",
    "    aux_loader = aux_datamgr.get_data_loader(train_file, aug=False)\n",
    "    iterator = tqdm(enumerate(classical_train_loader, start=1),\n",
    "                        leave=False, file=sys.stdout, position=0)\n",
    "    aux_iterator = iter(aux_loader)\n",
    "    \n",
    "    \n",
    "    # compute f, grad\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    gradient = np.zeros(n_param)\n",
    "    n_batches = 0\n",
    "    total_loss = 0.\n",
    "    curr = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param_len = len(param.flatten())\n",
    "            param.data = torch.tensor(\n",
    "                x[curr:curr+param_len].astype(np.float32)).cuda().reshape(param.shape)\n",
    "            curr += param_len\n",
    "    \n",
    "    for i, batch in iterator:\n",
    "        \n",
    "        # update L\n",
    "        aux_batch_x, aux_batch_y = next(aux_iterator)\n",
    "        aux_batch_x = aux_batch_x.cuda()\n",
    "        aux_batch_y = aux_batch_y.cuda()\n",
    "        aux_features_x = model(aux_batch_x, features_only=True)\n",
    "#         print(\"aux_features_x\", aux_features_x.shape)\n",
    "        model.module.fc.update_L(aux_features_x, aux_batch_y)\n",
    "           \n",
    "        # loss gradient \n",
    "        batch_x, batch_y = batch\n",
    "        batch_x = batch_x.cuda()\n",
    "        batch_y = batch_y.cuda()\n",
    "        features_x = model(batch_x, features_only=True)\n",
    "#         print(\"features_x\", features_x.shape)\n",
    "        output_x = model.module.fc(features_x)\n",
    "        loss = -loss_func(output_x, batch_y)   \n",
    "#         print(output_x, batch_y)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()   \n",
    "        loss.backward()\n",
    "\n",
    "        # statitics\n",
    "        curr = 0\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad:\n",
    "                param_len = len(param.flatten())\n",
    "                grad = param.grad.flatten().cpu().numpy()\n",
    "                assert grad.shape == param.flatten().shape\n",
    "                gradient[curr:curr+param_len] += grad\n",
    "                curr += param_len\n",
    "        n_batches += 1\n",
    "        \n",
    "    gradient /= n_batches\n",
    "    total_loss /= n_batches\n",
    "    print(f\"loss: {total_loss}\")\n",
    "    return total_loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "38650325-5097-4a54-a868-4bdf47e236de"
    }
   },
   "outputs": [],
   "source": [
    "def compute_sharpness(model, eps=0.0005):\n",
    "    \n",
    "    n_param = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            n_param += len(param.flatten()) \n",
    "    print(f\"no of parameters: {n_param}\")\n",
    "    \n",
    "    # x0\n",
    "    x0 = np.zeros(n_param)\n",
    "    curr = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param_len = len(param.flatten())\n",
    "            x0[curr:curr+param_len] = param.flatten().detach().cpu().numpy()\n",
    "            curr += param_len\n",
    "    \n",
    "    # bounds\n",
    "    bounds = np.zeros((n_param, 2))\n",
    "    bounds[:, 0] = -1e9\n",
    "    bounds[:, 1] = 1e9\n",
    "    curr = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param_len = len(param.flatten())\n",
    "            val0 = x0[curr:curr+param_len]\n",
    "            bounds[curr:curr+param_len, 0] = val0 - eps * (np.abs(val0) + 1)\n",
    "            bounds[curr:curr+param_len, 1] = val0 + eps * (np.abs(val0) + 1)\n",
    "            curr += param_len\n",
    "    \n",
    "    # f_x0\n",
    "    f_x0, _ = func(x0, n_param, model)\n",
    "    f_x0 = -f_x0\n",
    "    \n",
    "    # L-BFGS-B optimizer\n",
    "    x_optimal, y_opt, opt_info = scipy.optimize.fmin_l_bfgs_b(\n",
    "        func=func, x0=x0, args=(n_param, model), bounds=bounds)\n",
    "    sharpness = (-y_opt - f_x0) * 100. / (1 + f_x0) \n",
    "    print(f\"sharpness : {sharpness}\")\n",
    "    print(x_optimal, y_opt, opt_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbpresent": {
     "id": "c9bd3867-094b-484b-8f4b-5ffb161d9660"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of parameters: 12465283\n",
      "Accessing function value\n",
      "loss: -0.012343934960663319\n",
      "Accessing function value\n",
      "loss: -0.012672632249693077\n",
      "Accessing function value\n",
      "loss: -1.1614664324124655\n",
      "Accessing function value\n",
      "loss: -6.457420837084452\n",
      "Accessing function value\n",
      "loss: -6.537908771832784\n",
      "Accessing function value\n",
      "loss: -6.542549200057984\n",
      "Accessing function value\n",
      "loss: -6.678083092371622\n",
      "Accessing function value\n",
      "loss: -6.64824382464091 \n",
      "Accessing function value\n",
      "loss: -6.667147625287374\n",
      "Accessing function value\n",
      "loss: -6.604393127759297\n",
      "Accessing function value\n",
      "loss: -6.5696873299280805\n",
      "Accessing function value\n",
      "loss: -6.575246748924255\n",
      "Accessing function value\n",
      "loss: -6.565065883000692\n",
      "Accessing function value\n",
      "loss: -6.585975462595622\n",
      "sharpness : 649.3476476342391\n",
      "[ 1.00000000e+00 -1.80262337e-05 -5.32375868e-05 ...  8.35407682e-02\n",
      " -1.17586164e-03 -1.08974662e-02] -6.585975462595622 {'grad': array([ 0.00000000e+00,  2.82697728e-08,  2.73237390e-08, ...,\n",
      "       -1.77062696e-04, -7.49200914e-07,  3.32731819e-04]), 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'funcalls': 13, 'nit': 6, 'warnflag': 0}\n"
     ]
    }
   ],
   "source": [
    "compute_sharpness(model_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no of parameters: 12465283\n",
    "Accessing function value\n",
    "loss: -0.010902153588831424\n",
    "Accessing function value\n",
    "loss: -0.010902153626084328\n",
    "Accessing function value\n",
    "loss: -2.16028910279274 \n",
    "Accessing function value\n",
    "loss: -29.300055592854818\n",
    "Accessing function value\n",
    "loss: -35.17557498931885\n",
    "Accessing function value\n",
    "loss: -39.13319180806478\n",
    "Accessing function value\n",
    "loss: -40.946407178243  \n",
    "Accessing function value\n",
    "loss: -42.0526579284668 \n",
    "Accessing function value\n",
    "loss: -42.95348759969075\n",
    "Accessing function value\n",
    "loss: -44.69223894755046\n",
    "Accessing function value\n",
    "loss: -45.28219554901123\n",
    "Accessing function value\n",
    "loss: -45.68622080485026\n",
    "Accessing function value\n",
    "loss: -46.00641686757405\n",
    "Accessing function value\n",
    "loss: -46.649851417541505\n",
    "Accessing function value\n",
    "loss: -47.029435335795085\n",
    "Accessing function value\n",
    "loss: -47.5339617284139 \n",
    "Accessing function value\n",
    "loss: -47.75037869771322\n",
    "Accessing function value\n",
    "loss: -47.889085896809895\n",
    "Accessing function value\n",
    "loss: -48.011613032023114\n",
    "Accessing function value\n",
    "loss: -48.76787291208903\n",
    "Accessing function value\n",
    "loss: -49.161722094217936\n",
    "Accessing function value\n",
    "loss: -49.52843022664388\n",
    "Accessing function value\n",
    "loss: -50.08370721181234\n",
    "Accessing function value\n",
    "loss: -50.514968897501625\n",
    "Accessing function value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml\n",
    "no of parameters: 12465283\n",
    "Accessing function value\n",
    "loss: -0.012894223866363366\n",
    "Accessing function value\n",
    "loss: -0.012551092219849428\n",
    "Accessing function value\n",
    "loss: -3.9413750688234965\n",
    "Accessing function value\n",
    "loss: -6.741963555018107\n",
    "Accessing function value\n",
    "loss: -6.796368522644043\n",
    "Accessing function value\n",
    "loss: -6.719416332244873\n",
    "Accessing function value\n",
    "loss: -6.719123921394348\n",
    "Accessing function value\n",
    "loss: -6.73290487130483 \n",
    "Accessing function value\n",
    "loss: -6.703963958422343\n",
    "Accessing function value\n",
    "loss: -6.8107650820414225\n",
    "Accessing function value\n",
    "loss: -6.808843242327372\n",
    "Accessing function value\n",
    "loss: -6.690572388966879\n",
    "sharpness : 659.2670792030835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "66 /1.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true = np.arange(0,10,0.1)\n",
    "m_true = 2.5\n",
    "b_true = 1.0\n",
    "y_true = m_true*x_true + b_true\n",
    "\n",
    "def func(params, *args):\n",
    "    print(\"calling f\")\n",
    "    x = args[0]\n",
    "    y = args[1]\n",
    "    m, b = params\n",
    "    y_model = m*x+b\n",
    "    error = y-y_model\n",
    "    return sum(error**2)\n",
    "\n",
    "initial_values = np.array([1.0, 0.0])\n",
    "mybounds = [(None,2), (None,None)]\n",
    "\n",
    "scipy.optimize.fmin_l_bfgs_b(func, x0=initial_values, args=(x_true,y_true), approx_grad=True)\n",
    "scipy.optimize.fmin_l_bfgs_b(func, x0=initial_values, args=(x_true, y_true), bounds=mybounds, approx_grad=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
