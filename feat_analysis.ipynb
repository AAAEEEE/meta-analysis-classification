{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import torch\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm \n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "import glob\n",
    "import pickle\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maml.datasets.miniimagenet import MiniimagenetMetaDataset, Task\n",
    "from maml.models.gated_conv_net_original import ImpRegConvModel\n",
    "from maml.models.conv_embedding_model import RegConvEmbeddingModel\n",
    "from maml.logistic_regression_utils import logistic_regression_grad_with_respect_to_w, logistic_regression_hessian_pieces_with_respect_to_w, logistic_regression_hessian_with_respect_to_w, logistic_regression_mixed_derivatives_with_respect_to_w_then_to_X\n",
    "from maml.logistic_regression_utils import logistic_regression_mixed_derivatives_with_respect_to_w_then_to_X_left_multiply\n",
    "from maml.algorithm import MetaOptnet, ProtoNet, ImpRMAML_inner_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_files = glob.glob('inner_solvers_features/**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inner_solvers_features/minim_5w5s_protonet_features_dict.pkl',\n",
       " 'inner_solvers_features/impregmaml_minim_5w1s_protonet_nolastact.pkl',\n",
       " 'inner_solvers_features/minim_5w1s_protonet_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w1s_SVM_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w1s_LR_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w1s_protosvm_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w5s_protosvm_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w5s_LR_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w5s_SVM_features_dict.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "for ff in all_features_files:\n",
    "    with open(ff, 'rb') as f:\n",
    "        all_features.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modular re-usable methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_variance(X):\n",
    "    \"\"\"\n",
    "        X: (N x d)\n",
    "        returns scalar\n",
    "        sum (||X - mean_X||_2^2) / N\n",
    "    \"\"\"\n",
    "#     print(f\"recvd X of shape {X.shape}\")\n",
    "    N = X.shape[0]\n",
    "    mean = X.mean(0)\n",
    "    return mean, (np.sum((X - X.mean(0))**2))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_from_features_dict(features_dict):\n",
    "    X = []\n",
    "    y = []\n",
    "    for label in features_dict.keys():\n",
    "        X.append(features_dict[label])\n",
    "        y += [label] * X[-1].shape[0]\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    y = np.array(y)\n",
    "#     print(f\"Finally returning X, y of shapes : {X.shape} and {y.shape}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCA(X):\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics to evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interclass_vs_intraclass variance\n",
    "def feature_clustering(features_dict, split):\n",
    "    X, y = get_X_y_from_features_dict(features_dict[split])\n",
    "    all_labels = set(y)\n",
    "    means = []\n",
    "    numerator = 0.\n",
    "    for label in all_labels:\n",
    "        mean, numerator_var = compute_mean_variance(X[y==label, :])\n",
    "        means.append(mean)\n",
    "        numerator += numerator_var\n",
    "    _, denominator = compute_mean_variance(np.stack(means, axis=0))\n",
    "    print(\"num\", numerator)\n",
    "    print(\"denom\", denominator)\n",
    "    return (numerator / (denominator * len(all_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance explained by top k components\n",
    "def variance_explained(features_dict, split):\n",
    "    X, y = get_X_y_from_features_dict(features_dict[split])\n",
    "    all_labels = set(y)\n",
    "    var_explained = []\n",
    "    for label in all_labels:\n",
    "        pca = get_PCA(X[y==label, :])\n",
    "        var_explained.append(pca.explained_variance_ratio_)\n",
    "    var_explained = (np.stack(var_explained, axis=0)).mean(0)\n",
    "    return var_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance in disc direction for classes 65 and 70 (can be averaged over multiple ones)\n",
    "def variance_discr_direction(features_dict, split):\n",
    "    X, y = get_X_y_from_features_dict(features_dict[split])\n",
    "    all_labels = set(y)\n",
    "    n_runs = 20\n",
    "    avg_var = 0.\n",
    "    for _ in range(n_runs):\n",
    "        binary_problem_labels = np.random.choice(\n",
    "            list(all_labels), 2, replace=False)\n",
    "        var_explained = []\n",
    "        X_1 = X[y==binary_problem_labels[0], :]\n",
    "        X_2 = X[y==binary_problem_labels[1], :]\n",
    "        y_bin = np.array([0] * len(X_1) + [1] * len(X_2))\n",
    "        with warnings.catch_warnings(record=True) as wn:\n",
    "            lr_classifier = LogisticRegression()\n",
    "            lr_classifier.fit(np.concatenate([X_1, X_2], axis=0), y_bin)\n",
    "        normalized_lr_classifier = lr_classifier.coef_.T / np.linalg.norm(lr_classifier.coef_.T)\n",
    "#         print(np.eye(normalized_lr_classifier.shape[0]))\n",
    "#         _, var_1 = compute_mean_variance(X_1 @ (np.eye(normalized_lr_classifier.shape[0]) - normalized_lr_classifier @ normalized_lr_classifier.T))\n",
    "#         _, var_2 = compute_mean_variance(X_2 @ (np.eye(normalized_lr_classifier.shape[0]) - normalized_lr_classifier @ normalized_lr_classifier.T))\n",
    "        _, var_1 = compute_mean_variance(X_1 @ normalized_lr_classifier)\n",
    "        _, var_2 = compute_mean_variance(X_2 @ normalized_lr_classifier)\n",
    "        _, tvar_1 = compute_mean_variance(X_1)\n",
    "        _, tvar_2 = compute_mean_variance(X_2)\n",
    "        avg_var += (var_1 / tvar_1 + var_2 / tvar_2) / 2.\n",
    "    print(\"correct\", avg_var/n_runs)\n",
    "    return avg_var / n_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ineq(a, b):\n",
    "    diff = a - b\n",
    "#     num = np.linalg.norm(diff[0, :] - diff[1, :])\n",
    "    diff = diff / np.linalg.norm(diff, axis=1)[:, None]\n",
    "    return diff[0, :].T @ diff[1, :] \n",
    "#     deno = np.sum(np.linalg.norm(diff, axis=1))\n",
    "#     return num / deno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperplane_variance(features_dict, split):\n",
    "    X, y = get_X_y_from_features_dict(features_dict[split])\n",
    "    all_labels = set(y)\n",
    "    n_runs = 50\n",
    "    print(\"n_runs\", n_runs)\n",
    "    r_hv = []\n",
    "    for _ in range(n_runs):\n",
    "        binary_problem_labels = np.random.choice(\n",
    "            list(all_labels), 2, replace=False)\n",
    "        X_1 = X[y==binary_problem_labels[0], :]\n",
    "        X_2 = X[y==binary_problem_labels[1], :]\n",
    "        \n",
    "        rhv_pair_classes = 0.\n",
    "        n_inner_runs = 20\n",
    "        for _ in range(n_inner_runs):\n",
    "            random_indices_1 = np.random.choice(len(X_1), 2, replace=False)\n",
    "            random_indices_2 = np.random.choice(len(X_2), 2, replace=False)\n",
    "            rhv_pair_classes += evaluate_ineq(X_1[random_indices_1, :], X_2[random_indices_2, :])\n",
    "        r_hv.append(rhv_pair_classes / n_inner_runs)\n",
    "        \n",
    "    return np.mean(r_hv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variaance for each indi. task\n",
    "def task_variance(features_dict, split):\n",
    "    X, y = get_X_y_from_features_dict(features_dict[split])\n",
    "    all_labels = set(y)\n",
    "    n_runs = 20\n",
    "    avg_var = 0.\n",
    "    for _ in range(n_runs):\n",
    "        binary_problem_labels = np.random.choice(\n",
    "            list(all_labels), 2, replace=False)\n",
    "        var_explained = []\n",
    "        X_1 = X[y==binary_problem_labels[0], :]\n",
    "        X_2 = X[y==binary_problem_labels[1], :]\n",
    "        y_bin = np.array([0] * len(X_1) + [1] * len(X_2))\n",
    "        with warnings.catch_warnings(record=True) as wn:\n",
    "            lr_classifier = LogisticRegression()\n",
    "            lr_classifier.fit(np.concatenate([X_1, X_2], axis=0), y_bin)\n",
    "        _, var_1 = compute_mean_variance(X_1 @ lr_classifier.coef_.T)\n",
    "        _, var_2 = compute_mean_variance(X_2 @ lr_classifier.coef_.T)\n",
    "        _, tvar_1 = compute_mean_variance(X_1)\n",
    "        _, tvar_2 = compute_mean_variance(X_2)\n",
    "        avg_var += (var_1 / tvar_1 + var_2 / tvar_2) / 2.\n",
    "    return avg_var / n_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main analysis engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = {\n",
    "    'fc': feature_clustering,\n",
    "#     'var_exp': variance_explained,\n",
    "#     'var_disc': variance_discr_direction,\n",
    "#     'per_task_variance': task_variance,\n",
    "#       'hyperplane_variance' : hyperplane_variance\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis: fc\n",
      "5w5s protonet\n",
      "num 45239.603125\n",
      "denom 408.4081726074219\n",
      "minim 5w1s\n",
      "num 18766.8975\n",
      "denom 222.35250854492188\n",
      "5w1s protonet\n",
      "num 17598.850729166665\n",
      "denom 218.79690551757812\n",
      "5w1s SVM\n",
      "num 162.87037923177084\n",
      "denom 2.6231672763824463\n",
      "5w1s LR\n",
      "num 879.0422591145832\n",
      "denom 12.17524528503418\n",
      "5w1s protosvm\n",
      "num 11097.01125\n",
      "denom 156.21510314941406\n",
      "5w5s protosvm\n",
      "num 18832.833749999998\n",
      "denom 192.88168334960938\n",
      "5w5s LR\n",
      "num 1547.010279947917\n",
      "denom 12.673722267150879\n",
      "5w5s SVM\n",
      "num 117.93078328450521\n",
      "denom 1.5197663307189941\n",
      "'Results:'\n",
      "{'5w1s LR': 4.512446353930455,\n",
      " '5w1s SVM': 3.880575514049515,\n",
      " '5w1s protonet': 5.027165114474384,\n",
      " '5w1s protosvm': 4.43979608336354,\n",
      " '5w5s LR': 7.629024879876971,\n",
      " '5w5s SVM': 4.849873172143869,\n",
      " '5w5s protonet': 6.9231601739526925,\n",
      " '5w5s protosvm': 6.102456640434457,\n",
      " 'minim 5w1s': 5.275097193306603}\n"
     ]
    }
   ],
   "source": [
    "for analysis_name, analysis_func in engine.items():\n",
    "    metrics = {}\n",
    "    print(f\"Running analysis: {analysis_name}\")\n",
    "    for i, (feature_name, features) in enumerate(zip(all_features_files, all_features)):  \n",
    "        name = \" \".join(feature_name.split('/')[-1].split('.')[0].split('_')[1:3])\n",
    "        print(name)\n",
    "        metrics[name] = engine[analysis_name](all_features[i], 'val')\n",
    "    pprint.pprint(f\"Results:\") \n",
    "    pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-744ef1a283f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'5w1s'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for name, value in metrics.items():\n",
    "    if '5w1s' in name:\n",
    "        plt.plot(value[:10], label=name, marker='o')\n",
    "    print(name, sum(value[:300]))\n",
    "plt.xticks(np.arange(0, 10), size=8)\n",
    "plt.yticks(np.arange(0, 0.2, 0.02), size=8)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
