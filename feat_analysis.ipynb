{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import torch\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm \n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "import glob\n",
    "import pickle\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maml.datasets.miniimagenet import MiniimagenetMetaDataset, Task\n",
    "from maml.models.gated_conv_net_original import ImpRegConvModel\n",
    "from maml.models.conv_embedding_model import RegConvEmbeddingModel\n",
    "from maml.logistic_regression_utils import logistic_regression_grad_with_respect_to_w, logistic_regression_hessian_pieces_with_respect_to_w, logistic_regression_hessian_with_respect_to_w, logistic_regression_mixed_derivatives_with_respect_to_w_then_to_X\n",
    "from maml.logistic_regression_utils import logistic_regression_mixed_derivatives_with_respect_to_w_then_to_X_left_multiply\n",
    "from maml.algorithm import MetaOptnet, ProtoNet, ImpRMAML_inner_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_files = glob.glob('inner_solvers_features/**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inner_solvers_features/minim_5w5s_protonet_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_10w1s_SVM_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w1s_protonet_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_10w1s_protonet_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w15s_protonet_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w1s_SVM_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w1s_LR_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w1s_protosvm_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_20w5s_protonet_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w15s_LR_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w5s_protosvm_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w5s_LR_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_5w5s_SVM_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_20w1s_SVM_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_10w1s_LR_features_dict.pkl',\n",
       " 'inner_solvers_features/minim_10w5s_protonet_features_dict.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "for ff in all_features_files:\n",
    "    with open(ff, 'rb') as f:\n",
    "        all_features.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modular re-usable methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_variance(X):\n",
    "    \"\"\"\n",
    "        X: (N x d)\n",
    "        returns scalar\n",
    "        sum (||X - mean_X||_2^2) / N\n",
    "    \"\"\"\n",
    "#     print(f\"recvd X of shape {X.shape}\")\n",
    "    N = X.shape[0]\n",
    "    mean = X.mean(0)\n",
    "    return mean, (np.sum((X - X.mean(0))**2))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_from_features_dict(features_dict):\n",
    "    X = []\n",
    "    y = []\n",
    "    for label in features_dict.keys():\n",
    "        X.append(features_dict[label])\n",
    "        y += [label] * X[-1].shape[0]\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    y = np.array(y)\n",
    "#     print(f\"Finally returning X, y of shapes : {X.shape} and {y.shape}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCA(X):\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variance(estimators):\n",
    "    estimators = np.concatenate(estimators, axis=0)\n",
    "    assert len(estimators.shape) == 2\n",
    "    assert estimators.shape[1] == 1600\n",
    "    _explained_variance_ratio = []\n",
    "#     print(\"final esitimators\", estimators.shape)\n",
    "#     estimators = estimators - estimators.mean(0)\n",
    "    pca = PCA(n_components=5)\n",
    "    pca.fit(estimators)\n",
    "#         print(f\"class: {i+1} exp. variance: \", S.explained_variance_ratio_)\n",
    "#         print(f\"class: {i+1} svd values : \", S.singular_values_)\n",
    "#     print(np.sum(pca.explained_variance_ratio_))\n",
    "    return pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics to evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interclass_vs_intraclass variance\n",
    "def feature_clustering(features_dict, split):\n",
    "    X, y = get_X_y_from_features_dict(features_dict[split])\n",
    "    all_labels = set(y)\n",
    "    means = []\n",
    "    numerator = 0.\n",
    "    for label in all_labels:\n",
    "        mean, numerator_var = compute_mean_variance(X[y==label, :])\n",
    "        means.append(mean)\n",
    "        numerator += numerator_var\n",
    "    _, denominator = compute_mean_variance(np.stack(means, axis=0))\n",
    "    print(\"num\", numerator)\n",
    "    print(\"denom\", denominator)\n",
    "    return (numerator / (denominator * len(all_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance explained by top k components\n",
    "def variance_explained(features_dict, split):\n",
    "    X, y = get_X_y_from_features_dict(features_dict[split])\n",
    "    all_labels = set(y)\n",
    "    var_explained = []\n",
    "    for label in all_labels:\n",
    "        pca = get_PCA(X[y==label, :])\n",
    "        var_explained.append(pca.explained_variance_ratio_)\n",
    "    var_explained = (np.stack(var_explained, axis=0)).mean(0)\n",
    "    return var_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance in disc direction for classes 65 and 70 (can be averaged over multiple ones)\n",
    "def variance_discr_direction(features_dict, split):\n",
    "    X, y = get_X_y_from_features_dict(features_dict[split])\n",
    "    all_labels = set(y)\n",
    "    n_runs = 20\n",
    "    avg_var = 0.\n",
    "    for _ in range(n_runs):\n",
    "        binary_problem_labels = np.random.choice(\n",
    "            list(all_labels), 2, replace=False)\n",
    "        var_explained = []\n",
    "        X_1 = X[y==binary_problem_labels[0], :]\n",
    "        X_2 = X[y==binary_problem_labels[1], :]\n",
    "        y_bin = np.array([0] * len(X_1) + [1] * len(X_2))\n",
    "        with warnings.catch_warnings(record=True) as wn:\n",
    "            lr_classifier = LogisticRegression()\n",
    "            lr_classifier.fit(np.concatenate([X_1, X_2], axis=0), y_bin)\n",
    "        normalized_lr_classifier = lr_classifier.coef_.T / np.linalg.norm(lr_classifier.coef_.T)\n",
    "#         print(np.eye(normalized_lr_classifier.shape[0]))\n",
    "#         _, var_1 = compute_mean_variance(X_1 @ (np.eye(normalized_lr_classifier.shape[0]) - normalized_lr_classifier @ normalized_lr_classifier.T))\n",
    "#         _, var_2 = compute_mean_variance(X_2 @ (np.eye(normalized_lr_classifier.shape[0]) - normalized_lr_classifier @ normalized_lr_classifier.T))\n",
    "        _, var_1 = compute_mean_variance(X_1 @ normalized_lr_classifier)\n",
    "        _, var_2 = compute_mean_variance(X_2 @ normalized_lr_classifier)\n",
    "        _, tvar_1 = compute_mean_variance(X_1)\n",
    "        _, tvar_2 = compute_mean_variance(X_2)\n",
    "        avg_var += (var_1 / tvar_1 + var_2 / tvar_2) / 2.\n",
    "    print(\"correct\", avg_var/n_runs)\n",
    "    return avg_var / n_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ineq(a, b):\n",
    "    diff = a - b\n",
    "#     num = np.linalg.norm(diff[0, :] - diff[1, :])\n",
    "    diff = diff / np.linalg.norm(diff, axis=1)[:, None]\n",
    "    return diff[0, :].T @ diff[1, :] \n",
    "#     deno = np.sum(np.linalg.norm(diff, axis=1))\n",
    "#     return num / deno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperplane_variance(features_dict, split):\n",
    "    X, y = get_X_y_from_features_dict(features_dict[split])\n",
    "    all_labels = set(y)\n",
    "    n_runs = 50\n",
    "    print(\"n_runs\", n_runs)\n",
    "    r_hv = []\n",
    "    for _ in range(n_runs):\n",
    "        binary_problem_labels = np.random.choice(\n",
    "            list(all_labels), 2, replace=False)\n",
    "        X_1 = X[y==binary_problem_labels[0], :]\n",
    "        X_2 = X[y==binary_problem_labels[1], :]\n",
    "        \n",
    "        rhv_pair_classes = 0.\n",
    "        n_inner_runs = 20\n",
    "        for _ in range(n_inner_runs):\n",
    "            random_indices_1 = np.random.choice(len(X_1), 2, replace=False)\n",
    "            random_indices_2 = np.random.choice(len(X_2), 2, replace=False)\n",
    "            rhv_pair_classes += evaluate_ineq(X_1[random_indices_1, :], X_2[random_indices_2, :])\n",
    "        r_hv.append(rhv_pair_classes / n_inner_runs)\n",
    "        \n",
    "    return np.mean(r_hv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variaance for each indi. task\n",
    "def task_variance(features_dict, split):\n",
    "    X, y = get_X_y_from_features_dict(features_dict[split])\n",
    "    all_labels = set(y)\n",
    "    n_runs = 20\n",
    "    avg_var = 0.\n",
    "    for _ in range(n_runs):\n",
    "        binary_problem_labels = np.random.choice(\n",
    "            list(all_labels), 2, replace=False)\n",
    "        var_explained = []\n",
    "        X_1 = X[y==binary_problem_labels[0], :]\n",
    "        X_2 = X[y==binary_problem_labels[1], :]\n",
    "        y_bin = np.array([0] * len(X_1) + [1] * len(X_2))\n",
    "        with warnings.catch_warnings(record=True) as wn:\n",
    "            lr_classifier = LogisticRegression()\n",
    "            lr_classifier.fit(np.concatenate([X_1, X_2], axis=0), y_bin)\n",
    "        _, var_1 = compute_mean_variance(X_1 @ lr_classifier.coef_.T)\n",
    "        _, var_2 = compute_mean_variance(X_2 @ lr_classifier.coef_.T)\n",
    "        _, tvar_1 = compute_mean_variance(X_1)\n",
    "        _, tvar_2 = compute_mean_variance(X_2)\n",
    "        avg_var += (var_1 / tvar_1 + var_2 / tvar_2) / 2.\n",
    "    return avg_var / n_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperplane_variance_plus(features_dict, split):\n",
    "    X, y = get_X_y_from_features_dict(features_dict[split])\n",
    "    all_labels = set(y)\n",
    "    outer_n_runs = 50\n",
    "    inner_n_runs = 25\n",
    "    print(\"outer_n_runs\", outer_n_runs)\n",
    "    print(\"inner_n_runs\", inner_n_runs)\n",
    "    explained_variance_ratio = []\n",
    "    for _1 in range(outer_n_runs):\n",
    "        \n",
    "        binary_problem_labels = np.random.choice(\n",
    "                list(all_labels), 2, replace=False)\n",
    "        X_1 = X[y==binary_problem_labels[0], :]\n",
    "        X_2 = X[y==binary_problem_labels[1], :]\n",
    "        estimators = []\n",
    "\n",
    "        for _2 in range(inner_n_runs):\n",
    "\n",
    "            random_train_indices_1 = np.random.choice(len(X_1), 5, replace=False)\n",
    "            random_train_indices_2 = np.random.choice(len(X_2), 5, replace=False)\n",
    "            with warnings.catch_warnings(record=True) as wn:\n",
    "                lr_classifier = LogisticRegression(penalty='none', max_iter=1000, tol=1e-6)\n",
    "                y_bin = np.array([0] * len(random_train_indices_1) + [1] * len(random_train_indices_2))\n",
    "                lr_classifier.fit(np.concatenate([X_1[random_train_indices_1, :],\n",
    "                                                  X_2[random_train_indices_2, :]], axis=0), y_bin)\n",
    "            normalized_lr_classifier = lr_classifier.coef_.T / np.linalg.norm(lr_classifier.coef_.T)\n",
    "            estimators.append(normalized_lr_classifier.T)\n",
    "        explained_variance_ratio.append(compute_variance(estimators))\n",
    "    return np.array(explained_variance_ratio)[:, :10].sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main analysis engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = {\n",
    "#     'fc': feature_clustering,\n",
    "#     'var_exp': variance_explained,\n",
    "#     'var_disc': variance_discr_direction,\n",
    "#     'per_task_variance': task_variance,\n",
    "#       'hyperplane_variance' : hyperplane_variance,\n",
    "    'hyperplane_variance_plus' : hyperplane_variance_plus\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analysis: hyperplane_variance_plus\n",
      "protonet 5w5s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.38291633527946084\n",
      "SVM 10w1s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.41569058564764966\n",
      "protonet 5w1s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.38108445831856924\n",
      "protonet 10w1s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.360159922816722\n",
      "protonet 5w15s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.3475063557229677\n",
      "SVM 5w1s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.4227811974310783\n",
      "LR 5w1s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.3644290450016169\n",
      "protosvm 5w1s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.37816641559344805\n",
      "protonet 20w5s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.38644055721630294\n",
      "LR 5w15s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.3500232227385873\n",
      "protosvm 5w5s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.35320573667161304\n",
      "LR 5w5s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.348460461812467\n",
      "SVM 5w5s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.39515958014920394\n",
      "SVM 20w1s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.4304870034413657\n",
      "LR 10w1s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.36888045539035647\n",
      "protonet 10w5s\n",
      "outer_n_runs 50\n",
      "inner_n_runs 25\n",
      "0.35559094212452746\n",
      "'Results:'\n",
      "{'LR 10w1s': 0.36888045539035647,\n",
      " 'LR 5w15s': 0.3500232227385873,\n",
      " 'LR 5w1s': 0.3644290450016169,\n",
      " 'LR 5w5s': 0.348460461812467,\n",
      " 'SVM 10w1s': 0.41569058564764966,\n",
      " 'SVM 20w1s': 0.4304870034413657,\n",
      " 'SVM 5w1s': 0.4227811974310783,\n",
      " 'SVM 5w5s': 0.39515958014920394,\n",
      " 'protonet 10w1s': 0.360159922816722,\n",
      " 'protonet 10w5s': 0.35559094212452746,\n",
      " 'protonet 20w5s': 0.38644055721630294,\n",
      " 'protonet 5w15s': 0.3475063557229677,\n",
      " 'protonet 5w1s': 0.38108445831856924,\n",
      " 'protonet 5w5s': 0.38291633527946084,\n",
      " 'protosvm 5w1s': 0.37816641559344805,\n",
      " 'protosvm 5w5s': 0.35320573667161304}\n"
     ]
    }
   ],
   "source": [
    "for analysis_name, analysis_func in engine.items():\n",
    "    metrics = {}\n",
    "    print(f\"Running analysis: {analysis_name}\")\n",
    "    for i, (feature_name, features) in enumerate(zip(all_features_files, all_features)):  \n",
    "        name = \" \".join(feature_name.split('/')[-1].split('.')[0].split('_')[1:3][::-1])\n",
    "        print(name)\n",
    "        metrics[name] = engine[analysis_name](all_features[i], 'test')\n",
    "        print(metrics[name])\n",
    "    pprint.pprint(f\"Results:\") \n",
    "    pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-744ef1a283f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'5w1s'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for name, value in metrics.items():\n",
    "    if '5w1s' in name:\n",
    "        plt.plot(value[:10], label=name, marker='o')\n",
    "    print(name, sum(value[:300]))\n",
    "plt.xticks(np.arange(0, 10), size=8)\n",
    "plt.yticks(np.arange(0, 0.2, 0.02), size=8)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
